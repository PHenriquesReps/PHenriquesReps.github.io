<html>
	<head>
		<title>Pedro Henriques</title>
		<link rel="icon" type="image/x-icon" href="/images/icon.png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/vs2015.min.css" />
		

		
	</head>
	<body class="single is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
                <header id="header">
                    <h1><a href="index.html">Pedro Henriques</a></h1>
                    <nav class="links">
                        <ul>
                            <li><a href="projects.html">Projects</a></li>
                            <li><a href="posts.html">Posts</a></li>
                            <li><a href="activities.html">Activities</a></li>								
                        </ul>
                    </nav>				
                </header>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<article class="post">
								<header>
									<div class="title">
										<h2><a href="#">Spotify End-to-End Project</a></h2>
									</div>
									<div class="meta">
										<time class="published" datetime="2023-10-21">October 21, 2023</time>
										<a href="#" class="author"><span class="name">Pedro Henriques</span><img src="images/Foto_Pass.jpeg" alt="" /></a>
									</div>
								</header>
								<span class="image featured"><img src="images/spotify.png" alt="" /></span>

								<p>
									In this project, we will build an ETL pipeline using Spotify API on AWS. 
									The pipeline will retrieve data from Spotify API, transform it to a desired format, and load it into an AWS data store. 
									You can access the entire code for this project on my GitHub Page - <a href="https://github.com/PHenriquesReps/spotify-end-to-end-dataengineering-project">Spotify End-to-End Project</a>

	
								</p>

								<h3>
									Objective
								</h3>

								<p>
									This project was developed with AWS services, to allow me to explore its products, and get familiar with it. In this project, I explore some functionalities, dependencies between services, and their connections. 
									With this project, I wanted to create a Data Engineer End-to-End Project, a fully automated production project, that extracts, transforms, and loads the data for analysis.
									<br>

									<b>Use Case:</b> Extract data from the artists, songs, and albums, that are present in the Global Top 50 playlist. This playlist is refreshed daily based on the streams of each track.
										
								</p>
								
								<h3>
									About the Dataset/API
								</h3>
                                <p>
									For this data we retrive data from the <a href="https://open.spotify.com/playlist/37i9dQZEVXbNG2KDcFcKOF">Spotify API</a>, about the albuns, songs, and asrtist that are present in the Global Top 50 playlist.
								</p>

								<h3>
									Services Used
								</h3>

								<p>
									<b>- Amazon S3 (Simple Storage Service):</b> is a highly scalable object storage service that can store and retrieve any data anywhere on the web. It is commonly used to store and distribute large media files, data backups, and static website files.
									<br><br>
									<b>- AWS Lambda:</b> is a serverless computing service that lets you run your code without managing servers. You can use Lambda to run code responding to events like changes in S3, DynamoDB, or other AWS services.
									<br><br>
									<b>- Amazon CloudWatch:</b> is a serverless computing service that lets you run your code without managing servers. You can use Lambda to run code responding to events like changes in S3, DynamoDB, or other AWS services.
									<br><br>
									<b>- AWS Glue Crawler:</b> is a fully managed service that automatically crawls your data sources, identifies data formats, and infers schemas to create an AWS Glue Data Catalog.
									<br><br>
									<b>- AWS Glue Data Catalog:</b> is a fully managed metadata repository that makes it easy to discover and manage data in AWS. You can use the Glue Data Catalog with other AWS services, such as Athena.
									<br><br>
									<b>- Amazon Athena:</b> is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. You can use Athena to analyze data in your Glue Data Catalog or in other S3 buckets.

								</p>

								<h3>
									Architecture
								</h3>

								<div>
									<img style=" display: block; width:65%; margin-left: auto; margin-right: auto;" src="images/spotify_architecture.png" alt="" />
									<br>
								</div>



								<h3>
									Project Execution Plan
								</h3>

								<p>
									<b>1- Create the bucket and the respective folder structure on S3</b>
									<br>
									For this structure, we create two main folders, the <em>raw_data</em> folder, and the <em>transformed_data</em> folder, to very suggestive names, that mention the data that will be stored inside of each.
									Inside the <em>raw_data</em> folder, we will have two more folders, the <em>to_process</em>, which will hold the raw data that was not processed yet, and the processed folder which contains the raw extracted data that was already processed.
									<br>
									Inside the <em>transformed_data</em> folder, we have three folders, representing the three instances we are pulling the data for analyses, Artists, Songs, and Albums.
								</p>

								<p>
									<b>2- Extract the data from Spotify API</b>
									<br>
									Create the request extract the data from the Spotify API, and store it in a raw format on the <em>to_process</em> folder.

									<br><br>

									In this part I worked with two different packages <em>spotipy</em> to connect and extract the data from Spotify and boto3 that is used to create, configure, and manage AWS services, In this particular project was used to interact with the S3 bucket created.
									<br>
									We create a Spotify object, that allows us to connect to Spotify and extract the data using its internal functions (<em>playlist_tracks</em>).
								</p>
									
								
<pre class="line-numbers"><code class="python"> 
client_credentials_manager = SpotifyClientCredentials(client_id = client_id, client_secret = client_secret)
sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)		# create the connected object 

data = sp.playlist_tracks(playlist_URI)		# pass the id of the playlist
		
</code></pre>

								<p>
									The boto3 object, that allow us to connect to the S3 bucket and load the raw data to a specific folder.
								</p>
<pre><code>
client = boto3.client('s3')

filename = 'spotify_raw_' + str(datetime.now()) + ".json"

client.put_object(
    Bucket='spotify-etl-project-ph',
    Key='raw_data/to_processe/' + filename,
    Body=json.dumps(data)
    )
	
</code></pre>



								<p>
									<b>3- Transform and Load</b>
									<br>
									First, we will see if exist files inside the <em>to_process</em> folder. If there are, we'll extract the data from each of them, one by one, and store it in a variable.
								
								</p>
								<pre><code>
s3 = boto3.client('s3')
Bucket='spotify-etl-project-ph'
Key='raw_data/to_processe/'
									
for file in s3.list_objects(Bucket=Bucket, Prefix=Key)['Contents']:
	file_key = file['Key']
	if file_key.split('.')[-1] == "json":
		response = s3.get_object(Bucket = Bucket, Key = file_key)       # get the file information
		content = response['Body']      # retrieve the data inside the Body
		jsonObject = json.loads(content.read())         # get the actual data
</code></pre>

								<p>
									Then from this data, we will retrieve the fields from the artists, songs, and albums to do further analyses.
								</p>

								<pre><code>
# Geting the albums data
def album(data):
	album_list = []
	for row in data['items']:
		album_id = row['track']['album']['id']
		album_name = row['track']['album']['name']
		album_release_date = row['track']['album']['release_date']
		album_total_tracks = row['track']['album']['total_tracks']
		album_url = row['track']['album']['external_urls']['spotify']
	
	#creating a dictionary to store the looped data
		album_elements= {'album_id': album_id, 'name': album_name, 'release_date': album_release_date, 
						 'total_tracks': album_total_tracks, 'url': album_url}
						 
	# add that data to the list
		album_list.append(album_elements)
	
	return album_list
</code></pre>



								<p>
									<b>4- Add Triggers</b>
									<br>
									The two triggers were added to the pipeline, automating the pipeline run, based on the required schedule (weekly), and on the existence of new data in the to_process folder. 
									<br><br>
									<b>EventBridge (CloudWatch Events)</b>, this first trigger is used to activate the lambda function that will extract the data from the Spotify API, on a weekly basis.
									<br><br>
									<b>S3</b> this trigger is launched when there are JSON files in the to_process folder and will activate the lambda function.
									<br><br>
									Note: To complete this step we need two different permissions: 
									<br>
									<em>AmazonS3FullAccess</em> - allows the connection between the lambda functions and the S3 buckets  
									<br>
									<em>AWSLambdaRole</em> - allows the two lambda functions to communicate with each other.
								</p>


								<p>
									<b>5/6- Create Crawler and the tables on AWS Glue Data Catalog</b>
									<br>
									The crawler is used to read the data from the transformed data sources and infer the schema and metadata for each of them. Creating the respective tables on the Data Catalog, based on those data sources.
								</p>


								<p>
									<b>7- Analyse the Data</b>
									<br>
									Querying the Data using AWS Athena, using plain SQL.
								</p>

								<h3>
									Conclusion
								</h3>
								<p>
									In this project I learn some fundamental concepts when working with AWS, allowing me to explore a lot of services available on the platform. 
									By architecting a robust, scalable, and efficient data extraction pipeline, we have harnessed the capabilities of AWS to gather, process, and analyze Spotify data.
									As we look ahead, this project serves as a testament to the endless possibilities that can be unlocked by harnessing the synergy of cloud computing and rich data sources like Spotify. 
									<br><br>
									Hope you have liked it. See you soon.
								</p>
<!--								

									<h4>Blockquote</h4>
									<blockquote>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. 
										Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan faucibus. 
										Vestibulum ante ipsum primis in faucibus lorem ipsum dolor sit amet nullam adipiscing eu felis.</blockquote>

									<h4>Preformatted</h4>
									
<pre><code>{'href': 'https://api.spotify.com/v1/playlists/37i9dQZEVXbNG2KDcFcKOF/tracks?offset=0&limit=100&additional_types=track',
'items': [{'added_at': '2023-10-20T08:54:06Z',
  'added_by': {'external_urls': {'spotify': 'https://open.spotify.com/user/'},
   'href': 'https://api.spotify.com/v1/users/',
   'id': '',
   'type': 'user',
   'uri': 'spotify:user:'},
  'is_local': False,
  'primary_color': None,
  'track': {'album': {'album_type': 'album',
	'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/4q3ewBCX7sLwd24euuV69X'},
	  'href': 'https://api.spotify.com/v1/artists/4q3ewBCX7sLwd24euuV69X',
	  'id': '4q3ewBCX7sLwd24euuV69X',
	  'name': 'Bad Bunny',
	  'type': 'artist',
	  'uri': 'spotify:artist:4q3ewBCX7sLwd24euuV69X'}]
</code></pre>

-->

						</article>

					</div>
					<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
					<script>hljs.initHighlightingOnLoad();</script>
					<script src="script.js"></script>
		
	</body>
</html>