<html>
	<head>
		<title>Pedro Henriques</title>
		<link rel="icon" type="image/x-icon" href="/images/icon.png">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="single is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
                <header id="header">
                    <h1><a href="index.html">Pedro Henriques</a></h1>
                    <nav class="links">
                        <ul>
                            <li><a href="projects.html">Projects</a></li>
                            <li><a href="posts.html">Posts</a></li>
                            <li><a href="activities.html">Activities</a></li>								
                        </ul>
                    </nav>				
                </header>

				<!-- Main -->
				<div id="main">

					<!-- Post -->
						<article class="post1">
							<header>
								<div class="title">
									<h2>Spark Architecture</h2>
								</div>
								
								<div class="meta">
									<time class="published" datetime="2023-10-21">October 21, 2023</time>
									<a href="index.html" class="author"><span class="name">Pedro Henriques</span><img src="images/Foto_Pass.jpeg" alt="" /></a>
								</div>
							</header>
							<span class="image featured1"><img src="images/spark_architecture.PNG" alt="" /></span>
							<p>
								In this post, I will talk about some knowledge and thoughts that I have collected recently about the Spark Architecture, we will be peeling back the layers to reveal the intricate framework that empowers it. 
                                Whether you're a data enthusiast or a seasoned professional, understanding Spark's inner workings is key to unlocking its full potential and harnessing the vast possibilities of big data analytics.
                            </p>
                            
                            <h3>
                                The Spark Cluster
                            </h3>
                            <p>
                                At the heart of Spark's architecture lies a cluster, often comprised of several components working in harmony to process and analyze vast amounts of data.

                                <br> <br>
                                For this architectural example, Iâ€™m using a Spark Cluster, with YARN for Resource Management, and HDFS as the storage layer.
                                <br> <br>
                                The Spark Cluster consists of one Master Node and multiple Worker Nodes. For the purpose of this example, we will assume that it is composed of 1 Master Node and 3 Worker Nodes. 
                                In every node/machine, we have services related to Resource Management (YARN) and Storage (HDFS), on the Master, we have the Resource Manager and Name Node, and on the Worker nodes, we have the Node Manager and Data Node respectively. 

							</p>

                            <h3>
                                The Spark Flow
                            </h3>

                            <section>
                                <div class="row">
                                    <div class="col-12-medium">
                                        <p>When a client sends a request to the Spark Cluster, that request will be handled by the Resource Manager on the Master Node. </p>
                                        <ul class="alt">
                                            <li>1-	The request is initially handled by the Resource Manager on the Master Node. It will interact with the Node Manager of a designated Worker Node (e.g., Worker Node 3). </li>
											<li>2-	The Node Manager creates a container/executor. This is where the Driver service will start. The Driver acts as a local manager, , managing the application in place of the Resource Manager. </li>
                                            <li>3-	The Driver communicates with the Name Node to identify the location of the data, in which Worker Nodes the data is kept. </li>
                                            <li>4-	The Name Node informs the Driver where the data is stored. </li>
                                            <li>5-	The Driver requests resources from the Resource Manager (more containers) for specific Worker Nodes that house the partitions/blocks of data (principal of data locality). </li>
                                            <li>6-	The Resource Manager creates containers/executors (give the resources) to the Worker Nodes. </li>
                                            <li>7-	These containers are then managed by the Node Manager, allowing for the execution of the Spark job. </li>
                                        
                                    </div>
                                </div>
                            </section>

                            <h3>
                                Modes of Operation
                            </h3>

                            <p>
                                Spark can run in 2 different modes:
                                <br>

                                <b>Client Mode:</b> Client mode occurs when the driver is running on the edge/gateway node. This mode is normally used for development and testing purposes. 
                                When we have the need to see the results immediately while we perform some transformations on the data. This is not recommended for production environments because since the driver is running on the edge node, outside the cluster, if the client machine is disconnected, shut down, or crashes the entire application will crash. (exp. when we use notebooks)
                                <br><br>

                                <b>Cluster Mode:</b> Cluster mode occurs when the driver is running as part of the cluster, in one of the worker nodes, as we see in the diagram above. 
                                This is more suitable for a production environment, where the client packages and deploys the code, and the application runs on the backend.
                            
                            </p>
                           

                            <p>
                                The application can also run in Uber Mode:
                                <br>
                                
                                <b>Uber Mode:</b> The Uber Mode occurs when we have a small-sized job to run, which can directly run on the same machine/node, using the container's resources where the Driver is kept. 
                                By doing that the Driver will not need to request more resources from the Resource Manager, freeing resources for other applications, speeding up the execution of these small jobs.

                            </p>
            
                            <p>
                                In conclusion, understanding Spark's architecture is a pivotal step towards harnessing its immense potential in the world of big data analytics. 
                                With an efficient cluster setup, precise resource management, and awareness of the appropriate operational mode, Spark can empower organizations to unlock insights and drive data-driven decisions. 
                                <br><br>
                                Hope you have enjoyed it. 
                            </p>



                </div>

            </div>
    </body>
</html>